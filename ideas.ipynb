{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recog1: Facial recognition\n",
    "This program only requires 1 image of a person to start identifying them.\n",
    "The simulated repository of the person can be updated to become more accurate.\n",
    "Great for security system where common persons are well documented and guests can be identified and recorded.\n",
    "\n",
    "0. INITIAL \n",
    "1. Get the face*\n",
    "    1. Get an image with at least 1 person in it\n",
    "    2. Use Yolo to identify person within image\n",
    "    3. Extract face from person within image\n",
    "2. Generate facial expressions\n",
    "    1. Predict and identify facial expression of face* based on neurla network\n",
    "    2. Use a facial generation program to generate range of expressions using face*\n",
    "    3. Store and separate difference between expression of face*(real) and generated faces (simulated)\n",
    "3. Generate 3D scan of each expression \n",
    "    1. For each expression, real and simulated, generate faces at different angles\n",
    "    2. Store and separate difference between expression of face*(real) and generated faces (simulated)\n",
    "4. UPDATE\n",
    "    1. Get an image iwth at least 1 person in it\n",
    "    2. Use Yolo to identify person within image\n",
    "    3. Extract face from person within image\n",
    "    4. if person exists, check if matching old REAL facial expression\n",
    "    5. if person exists, but no REAL facial expression found, replace simulated with real and regenerate 3D.\n",
    "\n",
    "\n",
    "USEFUL GITHUB REPOS\n",
    "Get the face\n",
    "https://cmusatyalab.github.io/openface/\n",
    "http://krasserm.github.io/2018/02/07/deep-face-recognition/\n",
    "Get the expressions\n",
    "https://github.com/JostineHo/mememoji\n",
    "https://github.com/thoughtworksarts/EmoPy\n",
    "https://github.com/foamliu/Facial-Expression-Prediction\n",
    "https://github.com/fengju514/Expression-Net\n",
    "Make the expressions\n",
    "https://github.com/HuiDingUMD/ExprGAN\n",
    "https://github.com/yunjey/StarGAN\n",
    "https://github.com/timxzz/GANs\n",
    "\n",
    "Make the 3D\n",
    "https://github.com/YadiraF/face3d\n",
    "https://github.com/fengju514/Face-Pose-Net\n",
    "\n",
    "1. Get 1 image of person\n",
    "2. Predict and identify facial expression based on neural network (e.g. happy, sad, content, angry, ...)\n",
    "3. Based on facial expression predicted, as well as visualization program, recreate 1 image for each other emotion.\n",
    "4. Generate simulated images of person's face \n",
    "5. Use simulated images of person's face as training data\n",
    "\n",
    "Major components of program\n",
    "* Facial expression recognition\n",
    "If you have a standardized set of major facial expresses that are possible with muscles in human face, one should be able to identify facial expressions.\n",
    "* Simulation of facial expression\n",
    "Have not found if this is available - how to make a sad person look happy and vice versa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Hardware security module\n",
    "Problem: How to secure crypto wallets. They can be hacked from hot wallets (on computer) and can be stolen from people (on hardware wallets). \n",
    "\n",
    "Solution: What if your private keys can be stored on a type of smart contract + identity verification.\n",
    "Store keys with a custodian - if there are any problems, the keys are destroyed.\n",
    "Custodians will not have access to keys, but rather be the gate keeper.\n",
    "For customers to get their key, they will have to use identity verification.\n",
    "If you can identify who you are you can recover your private key\n",
    "\n",
    "is this necessary?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy Weights\n",
    "Create a way for people to download pretrained weights so that anybody can do object detection without much programming knowledge - kind of like in the matrix \n",
    "\n",
    "Process\n",
    "Initial download:\n",
    "    image classifier skeleton\n",
    "    image weight defaults\n",
    "    \n",
    "Search option 1:\n",
    "    Environment (ex: nature, rural city, urban city, ...)\n",
    "\n",
    "Search option 2: \n",
    "    Specific objects (ex: table, flower, tiger, ...)\n",
    "\n",
    "\n",
    "```the gym holds all the weights```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment detection - context based detection\n",
    "* Notice objects \n",
    "* Ignore objects\n",
    "* train on background\n",
    "\n",
    "**Idea** \n",
    "* Use Image classification for environment/context based labeling\n",
    "* Based on label, use YOLO to extract region proposals and detect objects\n",
    "\n",
    "key items related to environment\n",
    "ex: Mountain, field, wheat, forest, canyon, city\n",
    "\n",
    "Major categories\n",
    "inside(within enclosure), outside(no enclosure)\n",
    "\n",
    "Image recognition should use databases pertaining to the major categories\n",
    "\n",
    "**Outside: Environment** labels denoting spacial-scapes\n",
    "* day sky\n",
    "* night sky\n",
    "* sun rise\n",
    "* sun set\n",
    "* large body of water\n",
    "* horizon\n",
    "\n",
    "**Outside: Objects** labels denoting objects that can only be found outside\n",
    "* sun\n",
    "* moon \n",
    "* hill\n",
    "* mountain\n",
    "* tree\n",
    "* field\n",
    "* forest\n",
    "* canyon\n",
    "* city\n",
    "* Building\n",
    "\n",
    "**Inside: Objects** labeled items that can only be found indoors.\n",
    "* Doors\n",
    "* Television\n",
    "* kitchen\n",
    "* bathroom\n",
    "* carpet\n",
    "* bed\n",
    "* ceiling \n",
    "* hardwood floor\n",
    "* tile floor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volume ephasized chart\n",
    "\n",
    "Separate based on candlestick time interval\n",
    "* weekly, bin= 6mo\n",
    "* daily, bin= 30d\n",
    "* hourly, bin= 24h\n",
    "\n",
    "Questions:\n",
    "    How do we eliminate the price discrepancies?\n",
    "    How do we remove fluctuation? - brownian?\n",
    "    How to visualize volume? thickness?\n",
    "    \n",
    "* Use average instad of candlestick.\n",
    "* Turn vertical thickness into volume, with midpoint being at average\n",
    "* For each weekly, daily, and hourly, sample randomely to create bins of 6mo, 30d, and 24h respectively.\n",
    "* Each time interval will have its own probabilities\n",
    "\n",
    "* After turning average chart into a image - \n",
    "    * Use inception V3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eye tracking & wavelength sensitivity\n",
    "* track eye movement\n",
    "* see where eye tracking is lost to determine wavelength sensitivity\n",
    "\n",
    "Question:\n",
    "    how do we get appropriate data for training?\n",
    "        Game? click in boxes, must look to click so every time click, then take picture\n",
    "\n",
    "for example: Have 3x3 grid. person clicks on a grid, picture is taken. picture is labeled and compared to grid clicked. This continues until each grid has been linked to a picture.\n",
    "\n",
    "If the grid is very fine, there will be a gridient of possible grids the person is looking at.\n",
    "\n",
    "* ogama (OpenGaze and Mouse Analyzer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
